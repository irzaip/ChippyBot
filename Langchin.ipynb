{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391693ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['SERPER_API_KEY'] = \"d204a3336877e5419f12e0e5d46ad6eb94e5dde1\"\n",
    "\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "search = GoogleSerperAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to ask with search\"\n",
    "    )\n",
    "]\n",
    "\n",
    "self_ask_with_search = initialize_agent(tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True)\n",
    "self_ask_with_search.run(\"mengapa Geoffrey Hinton keluar dari Google?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9299d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b48db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d86e11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d6156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "huggingfacehub_api_token = os.environ['HUGGINGFACEHUB_API']\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = os.environ['HUGGINGFACEHUB_API']\n",
    "\n",
    "repo_id = \"google/flan-t5-xl\" # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\":0, \"max_length\":64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9149d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "huggingfacehub_api_token = os.environ['HUGGINGFACEHUB_API']\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = os.environ['HUGGINGFACEHUB_API']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8246d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceEndpoint\n",
    "endpoint_url = (\n",
    "    \"https://abcdefghijklmnop.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    ")\n",
    "hf = HuggingFaceEndpoint(\n",
    "    endpoint_url=endpoint_url,\n",
    "    huggingfacehub_api_token=os.environ['HUGGINGFACEHUB_API']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65275410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "repo_id = \"OpenAssistant/galactica-6.7b-finetuned\"\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\":0, \"max_length\":64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932af98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"hallo, apa kabar?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b4498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e2a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97da2969",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91971d15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mE:\\anaconda3\\envs\\py3916\\lib\\site-packages\\langchain\\llms\\llamacpp.py:122\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[0;32m    124\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Llama(\n\u001b[0;32m    125\u001b[0m         model_path\u001b[38;5;241m=\u001b[39mmodel_path,\n\u001b[0;32m    126\u001b[0m         lora_base\u001b[38;5;241m=\u001b[39mlora_base,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         last_n_tokens_size\u001b[38;5;241m=\u001b[39mlast_n_tokens_size,\n\u001b[0;32m    139\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m CallbackManager([StreamingStdOutCallbackHandler()])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Verbose is required to pass to the callback manager\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Make sure the model path is correct for your system!\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaCpp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./ggml-model-q4_0.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\py3916\\lib\\site-packages\\pydantic\\main.py:340\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\py3916\\lib\\site-packages\\pydantic\\main.py:1103\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\py3916\\lib\\site-packages\\langchain\\llms\\llamacpp.py:141\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    124\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Llama(\n\u001b[0;32m    125\u001b[0m         model_path\u001b[38;5;241m=\u001b[39mmodel_path,\n\u001b[0;32m    126\u001b[0m         lora_base\u001b[38;5;241m=\u001b[39mlora_base,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         last_n_tokens_size\u001b[38;5;241m=\u001b[39mlast_n_tokens_size,\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import llama-cpp-python library. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install the llama-cpp-python library to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse this embedding model: pip install llama-cpp-python\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m     )\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load Llama model from path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python"
     ]
    }
   ],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# Verbose is required to pass to the callback manager\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./ggml-model-q4_0.bin\", callback_manager=callback_manager, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deedbc6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HuggingFaceHub' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m llm\u001b[38;5;241m=\u001b[39mHuggingFaceHub(repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-xl\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m1e-10\u001b[39m})\n\u001b[0;32m      9\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen was Google founded?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m(question))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HuggingFaceHub' object has no attribute 'run'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm=HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":1e-10})\n",
    "\n",
    "question = \"When was Google founded?\"\n",
    "\n",
    "print(llm.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76795863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
